; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v3 | FileCheck %s --check-prefixes=AVX2
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v4 | FileCheck %s --check-prefixes=AVX512

define i256 @PR173924(<8 x i256> %a0) {
; AVX2-LABEL: PR173924:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    movl {{[0-9]+}}(%rsp), %edx
; AVX2-NEXT:    movl {{[0-9]+}}(%rsp), %edi
; AVX2-NEXT:    vmovq {{.*#+}} xmm0 = mem[0],zero
; AVX2-NEXT:    movl {{[0-9]+}}(%rsp), %ecx
; AVX2-NEXT:    movl {{[0-9]+}}(%rsp), %r8d
; AVX2-NEXT:    movl {{[0-9]+}}(%rsp), %r10d
; AVX2-NEXT:    andl $1, %r10d
; AVX2-NEXT:    andl $1, %r9d
; AVX2-NEXT:    addl %r10d, %r9d
; AVX2-NEXT:    andl $1, %r8d
; AVX2-NEXT:    andl $1, %ecx
; AVX2-NEXT:    addl %r8d, %ecx
; AVX2-NEXT:    addl %r9d, %ecx
; AVX2-NEXT:    andl $1, %esi
; AVX2-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; AVX2-NEXT:    vmovq %xmm0, %r8
; AVX2-NEXT:    andl $1, %edi
; AVX2-NEXT:    andl $1, %edx
; AVX2-NEXT:    addl %edi, %edx
; AVX2-NEXT:    addl %esi, %edx
; AVX2-NEXT:    addl %edx, %r8d
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vmovdqa %xmm0, 16(%rax)
; AVX2-NEXT:    addl %ecx, %r8d
; AVX2-NEXT:    vmovd %r8d, %xmm0
; AVX2-NEXT:    vmovdqa %xmm0, (%rax)
; AVX2-NEXT:    retq
;
; AVX512-LABEL: PR173924:
; AVX512:       # %bb.0:
; AVX512-NEXT:    movq %rdi, %rax
; AVX512-NEXT:    movl {{[0-9]+}}(%rsp), %ecx
; AVX512-NEXT:    movl {{[0-9]+}}(%rsp), %edi
; AVX512-NEXT:    vmovq {{.*#+}} xmm0 = mem[0],zero
; AVX512-NEXT:    movl {{[0-9]+}}(%rsp), %edx
; AVX512-NEXT:    movl {{[0-9]+}}(%rsp), %r8d
; AVX512-NEXT:    movl {{[0-9]+}}(%rsp), %r10d
; AVX512-NEXT:    andl $1, %r10d
; AVX512-NEXT:    andl $1, %r9d
; AVX512-NEXT:    addl %r10d, %r9d
; AVX512-NEXT:    andl $1, %r8d
; AVX512-NEXT:    andl $1, %edx
; AVX512-NEXT:    addl %r8d, %edx
; AVX512-NEXT:    addl %r9d, %edx
; AVX512-NEXT:    andl $1, %esi
; AVX512-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; AVX512-NEXT:    vmovq %xmm0, %r8
; AVX512-NEXT:    andl $1, %edi
; AVX512-NEXT:    andl $1, %ecx
; AVX512-NEXT:    addl %edi, %ecx
; AVX512-NEXT:    addl %esi, %ecx
; AVX512-NEXT:    addl %edx, %ecx
; AVX512-NEXT:    addl %ecx, %r8d
; AVX512-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512-NEXT:    vmovdqa %xmm0, 16(%rax)
; AVX512-NEXT:    vmovd %r8d, %xmm0
; AVX512-NEXT:    vmovdqa %xmm0, (%rax)
; AVX512-NEXT:    retq
  %m = and <8 x i256> %a0, splat (i256 1)
  %r = call i256 @llvm.vector.reduce.add.v8i256(<8 x i256> %m)
  ret i256 %r
}
