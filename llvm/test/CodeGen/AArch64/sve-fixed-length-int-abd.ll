; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -aarch64-sve-vector-bits-min=256  < %s | FileCheck %s -check-prefixes=CHECK,VBITS_GE_256
; RUN: llc -aarch64-sve-vector-bits-min=512  < %s | FileCheck %s -check-prefixes=CHECK,VBITS_GE_512,VBITS_EQ_512
; RUN: llc -aarch64-sve-vector-bits-min=2048 < %s | FileCheck %s -check-prefixes=CHECK,VBITS_GE_512,VBITS_GE_2048

target triple = "aarch64-unknown-linux-gnu"

; Don't use SVE for 128-bit vectors.
define void @sabd_v16i8_v16i16(ptr %a, ptr %b) #0 {
; CHECK-LABEL: sabd_v16i8_v16i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ldr q0, [x0]
; CHECK-NEXT:    ldr q1, [x1]
; CHECK-NEXT:    sabd v0.16b, v0.16b, v1.16b
; CHECK-NEXT:    str q0, [x0]
; CHECK-NEXT:    ret
  %a.ld = load <16 x i8>, ptr %a
  %b.ld = load <16 x i8>, ptr %b
  %a.sext = sext <16 x i8> %a.ld to <16 x i16>
  %b.sext = sext <16 x i8> %b.ld to <16 x i16>
  %sub = sub <16 x i16> %a.sext, %b.sext
  %abs = call <16 x i16> @llvm.abs.v16i16(<16 x i16> %sub, i1 true)
  %trunc = trunc <16 x i16> %abs to <16 x i8>
  store <16 x i8> %trunc, ptr %a
  ret void
}

; Don't use SVE for 128-bit vectors.
define void @sabd_v16i8_v16i32(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v16i8_v16i32:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ldp d0, d1, [x0]
; VBITS_GE_256-NEXT:    ldp d2, d3, [x1]
; VBITS_GE_256-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_GE_256-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_GE_256-NEXT:    sabd v0.16b, v0.16b, v2.16b
; VBITS_GE_256-NEXT:    str q0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_GE_512-LABEL: sabd_v16i8_v16i32:
; VBITS_GE_512:       // %bb.0:
; VBITS_GE_512-NEXT:    ldr q0, [x0]
; VBITS_GE_512-NEXT:    ldr q1, [x1]
; VBITS_GE_512-NEXT:    sabd v0.16b, v0.16b, v1.16b
; VBITS_GE_512-NEXT:    str q0, [x0]
; VBITS_GE_512-NEXT:    ret
  %a.ld = load <16 x i8>, ptr %a
  %b.ld = load <16 x i8>, ptr %b
  %a.sext = sext <16 x i8> %a.ld to <16 x i32>
  %b.sext = sext <16 x i8> %b.ld to <16 x i32>
  %sub = sub <16 x i32> %a.sext, %b.sext
  %abs = call <16 x i32> @llvm.abs.v16i32(<16 x i32> %sub, i1 true)
  %trunc = trunc <16 x i32> %abs to <16 x i8>
  store <16 x i8> %trunc, ptr %a
  ret void
}

; Don't use SVE for 128-bit vectors.
define void @sabd_v16i8_v16i64(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v16i8_v16i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ldp s0, s1, [x0]
; VBITS_GE_256-NEXT:    ptrue p0.s, vl4
; VBITS_GE_256-NEXT:    ldp s2, s3, [x0, #8]
; VBITS_GE_256-NEXT:    ldp s4, s5, [x1, #8]
; VBITS_GE_256-NEXT:    ldp s6, s7, [x1]
; VBITS_GE_256-NEXT:    sshll v0.8h, v0.8b, #0
; VBITS_GE_256-NEXT:    sshll v1.8h, v1.8b, #0
; VBITS_GE_256-NEXT:    sshll v2.8h, v2.8b, #0
; VBITS_GE_256-NEXT:    sshll v3.8h, v3.8b, #0
; VBITS_GE_256-NEXT:    sshll v5.8h, v5.8b, #0
; VBITS_GE_256-NEXT:    sshll v4.8h, v4.8b, #0
; VBITS_GE_256-NEXT:    sshll v6.8h, v6.8b, #0
; VBITS_GE_256-NEXT:    sshll v7.8h, v7.8b, #0
; VBITS_GE_256-NEXT:    sshll v0.4s, v0.4h, #0
; VBITS_GE_256-NEXT:    sshll v1.4s, v1.4h, #0
; VBITS_GE_256-NEXT:    sshll v2.4s, v2.4h, #0
; VBITS_GE_256-NEXT:    sshll v3.4s, v3.4h, #0
; VBITS_GE_256-NEXT:    sshll v5.4s, v5.4h, #0
; VBITS_GE_256-NEXT:    sshll v4.4s, v4.4h, #0
; VBITS_GE_256-NEXT:    sshll v6.4s, v6.4h, #0
; VBITS_GE_256-NEXT:    sshll v7.4s, v7.4h, #0
; VBITS_GE_256-NEXT:    sabd v3.4s, v3.4s, v5.4s
; VBITS_GE_256-NEXT:    sabd v2.4s, v2.4s, v4.4s
; VBITS_GE_256-NEXT:    sabd v1.4s, v1.4s, v7.4s
; VBITS_GE_256-NEXT:    sabd v0.4s, v0.4s, v6.4s
; VBITS_GE_256-NEXT:    splice z2.s, p0, z2.s, z3.s
; VBITS_GE_256-NEXT:    splice z0.s, p0, z0.s, z1.s
; VBITS_GE_256-NEXT:    uzp1 z1.h, z2.h, z2.h
; VBITS_GE_256-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_GE_256-NEXT:    str q0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_EQ_512-LABEL: sabd_v16i8_v16i64:
; VBITS_EQ_512:       // %bb.0:
; VBITS_EQ_512-NEXT:    ldp d0, d1, [x0]
; VBITS_EQ_512-NEXT:    ldp d2, d3, [x1]
; VBITS_EQ_512-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_512-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_EQ_512-NEXT:    sabd v0.16b, v0.16b, v2.16b
; VBITS_EQ_512-NEXT:    str q0, [x0]
; VBITS_EQ_512-NEXT:    ret
;
; VBITS_GE_2048-LABEL: sabd_v16i8_v16i64:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ldr q0, [x0]
; VBITS_GE_2048-NEXT:    ldr q1, [x1]
; VBITS_GE_2048-NEXT:    sabd v0.16b, v0.16b, v1.16b
; VBITS_GE_2048-NEXT:    str q0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %a.ld = load <16 x i8>, ptr %a
  %b.ld = load <16 x i8>, ptr %b
  %a.sext = sext <16 x i8> %a.ld to <16 x i64>
  %b.sext = sext <16 x i8> %b.ld to <16 x i64>
  %sub = sub <16 x i64> %a.sext, %b.sext
  %abs = call <16 x i64> @llvm.abs.v16i64(<16 x i64> %sub, i1 true)
  %trunc = trunc <16 x i64> %abs to <16 x i8>
  store <16 x i8> %trunc, ptr %a
  ret void
}

define void @sabd_v32i8_v32i16(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v32i8_v32i16:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ldp q0, q1, [x0]
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    ldp q2, q3, [x1]
; VBITS_GE_256-NEXT:    sabd v1.16b, v1.16b, v3.16b
; VBITS_GE_256-NEXT:    sabd v0.16b, v0.16b, v2.16b
; VBITS_GE_256-NEXT:    splice z0.b, p0, z0.b, z1.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_GE_512-LABEL: sabd_v32i8_v32i16:
; VBITS_GE_512:       // %bb.0:
; VBITS_GE_512-NEXT:    ptrue p0.h, vl32
; VBITS_GE_512-NEXT:    ld1sb { z0.h }, p0/z, [x0]
; VBITS_GE_512-NEXT:    ld1sb { z1.h }, p0/z, [x1]
; VBITS_GE_512-NEXT:    sabd z0.h, p0/m, z0.h, z1.h
; VBITS_GE_512-NEXT:    st1b { z0.h }, p0, [x0]
; VBITS_GE_512-NEXT:    ret
  %a.ld = load <32 x i8>, ptr %a
  %b.ld = load <32 x i8>, ptr %b
  %a.sext = sext <32 x i8> %a.ld to <32 x i16>
  %b.sext = sext <32 x i8> %b.ld to <32 x i16>
  %sub = sub <32 x i16> %a.sext, %b.sext
  %abs = call <32 x i16> @llvm.abs.v32i16(<32 x i16> %sub, i1 true)
  %trunc = trunc <32 x i16> %abs to <32 x i8>
  store <32 x i8> %trunc, ptr %a
  ret void
}

define void @uabd_v32i8_v32i16(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: uabd_v32i8_v32i16:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ldp q0, q1, [x0]
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    ldp q2, q3, [x1]
; VBITS_GE_256-NEXT:    uabd v1.16b, v3.16b, v1.16b
; VBITS_GE_256-NEXT:    uabd v0.16b, v2.16b, v0.16b
; VBITS_GE_256-NEXT:    splice z0.b, p0, z0.b, z1.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_GE_512-LABEL: uabd_v32i8_v32i16:
; VBITS_GE_512:       // %bb.0:
; VBITS_GE_512-NEXT:    ptrue p0.h, vl32
; VBITS_GE_512-NEXT:    ld1b { z0.h }, p0/z, [x0]
; VBITS_GE_512-NEXT:    ld1b { z1.h }, p0/z, [x1]
; VBITS_GE_512-NEXT:    uabd z0.h, p0/m, z0.h, z1.h
; VBITS_GE_512-NEXT:    st1b { z0.h }, p0, [x0]
; VBITS_GE_512-NEXT:    ret
  %a.ld = load <32 x i8>, ptr %a
  %b.ld = load <32 x i8>, ptr %b
  %a.zext = zext <32 x i8> %a.ld to <32 x i16>
  %b.zext = zext <32 x i8> %b.ld to <32 x i16>
  %sub = sub <32 x i16> %a.zext, %b.zext
  %abs = call <32 x i16> @llvm.abs.v32i16(<32 x i16> %sub, i1 true)
  %trunc = trunc <32 x i16> %abs to <32 x i8>
  store <32 x i8> %trunc, ptr %a
  ret void
}

define void @sabd_v32i8_v32i32(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v32i8_v32i32:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ldp d0, d1, [x0]
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    ldp d2, d3, [x1, #16]
; VBITS_GE_256-NEXT:    ldp d4, d5, [x0, #16]
; VBITS_GE_256-NEXT:    ldp d6, d7, [x1]
; VBITS_GE_256-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_GE_256-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_GE_256-NEXT:    mov v4.d[1], v5.d[0]
; VBITS_GE_256-NEXT:    mov v6.d[1], v7.d[0]
; VBITS_GE_256-NEXT:    sabd v1.16b, v4.16b, v2.16b
; VBITS_GE_256-NEXT:    sabd v0.16b, v0.16b, v6.16b
; VBITS_GE_256-NEXT:    splice z0.b, p0, z0.b, z1.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_EQ_512-LABEL: sabd_v32i8_v32i32:
; VBITS_EQ_512:       // %bb.0:
; VBITS_EQ_512-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl16
; VBITS_EQ_512-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_512-NEXT:    sabd v1.16b, v1.16b, v3.16b
; VBITS_EQ_512-NEXT:    sabd v0.16b, v0.16b, v2.16b
; VBITS_EQ_512-NEXT:    splice z0.b, p0, z0.b, z1.b
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl32
; VBITS_EQ_512-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_EQ_512-NEXT:    ret
;
; VBITS_GE_2048-LABEL: sabd_v32i8_v32i32:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.s, vl32
; VBITS_GE_2048-NEXT:    ld1sb { z0.s }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1sb { z1.s }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    sabd z0.s, p0/m, z0.s, z1.s
; VBITS_GE_2048-NEXT:    st1b { z0.s }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %a.ld = load <32 x i8>, ptr %a
  %b.ld = load <32 x i8>, ptr %b
  %a.sext = sext <32 x i8> %a.ld to <32 x i32>
  %b.sext = sext <32 x i8> %b.ld to <32 x i32>
  %sub = sub <32 x i32> %a.sext, %b.sext
  %abs = call <32 x i32> @llvm.abs.v32i32(<32 x i32> %sub, i1 true)
  %trunc = trunc <32 x i32> %abs to <32 x i8>
  store <32 x i8> %trunc, ptr %a
  ret void
}

define void @sabd_v32i8_v32i64(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v32i8_v32i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ldp s0, s1, [x0]
; VBITS_GE_256-NEXT:    ptrue p0.s, vl4
; VBITS_GE_256-NEXT:    ldp s2, s3, [x0, #8]
; VBITS_GE_256-NEXT:    ldp s4, s5, [x0, #16]
; VBITS_GE_256-NEXT:    ldp s6, s7, [x0, #24]
; VBITS_GE_256-NEXT:    sshll v0.8h, v0.8b, #0
; VBITS_GE_256-NEXT:    ldp s16, s17, [x1, #8]
; VBITS_GE_256-NEXT:    sshll v1.8h, v1.8b, #0
; VBITS_GE_256-NEXT:    ldp s18, s19, [x1, #24]
; VBITS_GE_256-NEXT:    sshll v2.8h, v2.8b, #0
; VBITS_GE_256-NEXT:    ldp s20, s21, [x1, #16]
; VBITS_GE_256-NEXT:    sshll v3.8h, v3.8b, #0
; VBITS_GE_256-NEXT:    ldp s22, s23, [x1]
; VBITS_GE_256-NEXT:    sshll v4.8h, v4.8b, #0
; VBITS_GE_256-NEXT:    sshll v5.8h, v5.8b, #0
; VBITS_GE_256-NEXT:    sshll v6.8h, v6.8b, #0
; VBITS_GE_256-NEXT:    sshll v7.8h, v7.8b, #0
; VBITS_GE_256-NEXT:    sshll v18.8h, v18.8b, #0
; VBITS_GE_256-NEXT:    sshll v19.8h, v19.8b, #0
; VBITS_GE_256-NEXT:    sshll v17.8h, v17.8b, #0
; VBITS_GE_256-NEXT:    sshll v20.8h, v20.8b, #0
; VBITS_GE_256-NEXT:    sshll v21.8h, v21.8b, #0
; VBITS_GE_256-NEXT:    sshll v22.8h, v22.8b, #0
; VBITS_GE_256-NEXT:    sshll v23.8h, v23.8b, #0
; VBITS_GE_256-NEXT:    sshll v16.8h, v16.8b, #0
; VBITS_GE_256-NEXT:    sshll v0.4s, v0.4h, #0
; VBITS_GE_256-NEXT:    sshll v1.4s, v1.4h, #0
; VBITS_GE_256-NEXT:    sshll v2.4s, v2.4h, #0
; VBITS_GE_256-NEXT:    sshll v3.4s, v3.4h, #0
; VBITS_GE_256-NEXT:    sshll v4.4s, v4.4h, #0
; VBITS_GE_256-NEXT:    sshll v5.4s, v5.4h, #0
; VBITS_GE_256-NEXT:    sshll v6.4s, v6.4h, #0
; VBITS_GE_256-NEXT:    sshll v7.4s, v7.4h, #0
; VBITS_GE_256-NEXT:    sshll v18.4s, v18.4h, #0
; VBITS_GE_256-NEXT:    sshll v19.4s, v19.4h, #0
; VBITS_GE_256-NEXT:    sshll v17.4s, v17.4h, #0
; VBITS_GE_256-NEXT:    sshll v20.4s, v20.4h, #0
; VBITS_GE_256-NEXT:    sshll v21.4s, v21.4h, #0
; VBITS_GE_256-NEXT:    sshll v22.4s, v22.4h, #0
; VBITS_GE_256-NEXT:    sshll v23.4s, v23.4h, #0
; VBITS_GE_256-NEXT:    sshll v16.4s, v16.4h, #0
; VBITS_GE_256-NEXT:    sabd v7.4s, v7.4s, v19.4s
; VBITS_GE_256-NEXT:    sabd v6.4s, v6.4s, v18.4s
; VBITS_GE_256-NEXT:    sabd v5.4s, v5.4s, v21.4s
; VBITS_GE_256-NEXT:    sabd v4.4s, v4.4s, v20.4s
; VBITS_GE_256-NEXT:    sabd v3.4s, v3.4s, v17.4s
; VBITS_GE_256-NEXT:    sabd v2.4s, v2.4s, v16.4s
; VBITS_GE_256-NEXT:    sabd v1.4s, v1.4s, v23.4s
; VBITS_GE_256-NEXT:    sabd v0.4s, v0.4s, v22.4s
; VBITS_GE_256-NEXT:    splice z6.s, p0, z6.s, z7.s
; VBITS_GE_256-NEXT:    splice z4.s, p0, z4.s, z5.s
; VBITS_GE_256-NEXT:    splice z2.s, p0, z2.s, z3.s
; VBITS_GE_256-NEXT:    splice z0.s, p0, z0.s, z1.s
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    uzp1 z1.h, z6.h, z6.h
; VBITS_GE_256-NEXT:    uzp1 z3.h, z4.h, z4.h
; VBITS_GE_256-NEXT:    uzp1 z2.h, z2.h, z2.h
; VBITS_GE_256-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    uzp1 z3.b, z3.b, z3.b
; VBITS_GE_256-NEXT:    uzp1 z2.b, z2.b, z2.b
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    mov v3.d[1], v1.d[0]
; VBITS_GE_256-NEXT:    mov v0.d[1], v2.d[0]
; VBITS_GE_256-NEXT:    splice z0.b, p0, z0.b, z3.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_EQ_512-LABEL: sabd_v32i8_v32i64:
; VBITS_EQ_512:       // %bb.0:
; VBITS_EQ_512-NEXT:    ldp d0, d1, [x0]
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl16
; VBITS_EQ_512-NEXT:    ldp d2, d3, [x1, #16]
; VBITS_EQ_512-NEXT:    ldp d4, d5, [x0, #16]
; VBITS_EQ_512-NEXT:    ldp d6, d7, [x1]
; VBITS_EQ_512-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_512-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_EQ_512-NEXT:    mov v4.d[1], v5.d[0]
; VBITS_EQ_512-NEXT:    mov v6.d[1], v7.d[0]
; VBITS_EQ_512-NEXT:    sabd v1.16b, v4.16b, v2.16b
; VBITS_EQ_512-NEXT:    sabd v0.16b, v0.16b, v6.16b
; VBITS_EQ_512-NEXT:    splice z0.b, p0, z0.b, z1.b
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl32
; VBITS_EQ_512-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_EQ_512-NEXT:    ret
;
; VBITS_GE_2048-LABEL: sabd_v32i8_v32i64:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.d, vl32
; VBITS_GE_2048-NEXT:    ld1sb { z0.d }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1sb { z1.d }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    sabd z0.d, p0/m, z0.d, z1.d
; VBITS_GE_2048-NEXT:    st1b { z0.d }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %a.ld = load <32 x i8>, ptr %a
  %b.ld = load <32 x i8>, ptr %b
  %a.sext = sext <32 x i8> %a.ld to <32 x i64>
  %b.sext = sext <32 x i8> %b.ld to <32 x i64>
  %sub = sub <32 x i64> %a.sext, %b.sext
  %abs = call <32 x i64> @llvm.abs.v32i64(<32 x i64> %sub, i1 true)
  %trunc = trunc <32 x i64> %abs to <32 x i8>
  store <32 x i8> %trunc, ptr %a
  ret void
}

define void @sabd_v64i8_v64i64(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v64i8_v64i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    sub sp, sp, #96
; VBITS_GE_256-NEXT:    stp d15, d14, [sp, #32] // 16-byte Folded Spill
; VBITS_GE_256-NEXT:    stp d13, d12, [sp, #48] // 16-byte Folded Spill
; VBITS_GE_256-NEXT:    stp d11, d10, [sp, #64] // 16-byte Folded Spill
; VBITS_GE_256-NEXT:    stp d9, d8, [sp, #80] // 16-byte Folded Spill
; VBITS_GE_256-NEXT:    .cfi_def_cfa_offset 96
; VBITS_GE_256-NEXT:    .cfi_offset b8, -8
; VBITS_GE_256-NEXT:    .cfi_offset b9, -16
; VBITS_GE_256-NEXT:    .cfi_offset b10, -24
; VBITS_GE_256-NEXT:    .cfi_offset b11, -32
; VBITS_GE_256-NEXT:    .cfi_offset b12, -40
; VBITS_GE_256-NEXT:    .cfi_offset b13, -48
; VBITS_GE_256-NEXT:    .cfi_offset b14, -56
; VBITS_GE_256-NEXT:    .cfi_offset b15, -64
; VBITS_GE_256-NEXT:    ldp s0, s1, [x0, #32]
; VBITS_GE_256-NEXT:    ptrue p0.s, vl4
; VBITS_GE_256-NEXT:    ldp s2, s3, [x0, #40]
; VBITS_GE_256-NEXT:    mov w8, #32 // =0x20
; VBITS_GE_256-NEXT:    ldp s4, s5, [x0, #48]
; VBITS_GE_256-NEXT:    sshll v0.8h, v0.8b, #0
; VBITS_GE_256-NEXT:    sshll v1.8h, v1.8b, #0
; VBITS_GE_256-NEXT:    sshll v2.8h, v2.8b, #0
; VBITS_GE_256-NEXT:    sshll v3.8h, v3.8b, #0
; VBITS_GE_256-NEXT:    sshll v4.8h, v4.8b, #0
; VBITS_GE_256-NEXT:    sshll v5.8h, v5.8b, #0
; VBITS_GE_256-NEXT:    sshll v6.4s, v0.4h, #0
; VBITS_GE_256-NEXT:    sshll v0.4s, v1.4h, #0
; VBITS_GE_256-NEXT:    sshll v2.4s, v2.4h, #0
; VBITS_GE_256-NEXT:    sshll v3.4s, v3.4h, #0
; VBITS_GE_256-NEXT:    sshll v4.4s, v4.4h, #0
; VBITS_GE_256-NEXT:    sshll v5.4s, v5.4h, #0
; VBITS_GE_256-NEXT:    stp q0, q6, [sp] // 32-byte Folded Spill
; VBITS_GE_256-NEXT:    ldp s22, s23, [x0, #16]
; VBITS_GE_256-NEXT:    ldr s13, [x1, #20]
; VBITS_GE_256-NEXT:    ldp s6, s7, [x0, #56]
; VBITS_GE_256-NEXT:    ldr s14, [x1]
; VBITS_GE_256-NEXT:    ldp s16, s17, [x0]
; VBITS_GE_256-NEXT:    sshll v13.8h, v13.8b, #0
; VBITS_GE_256-NEXT:    ldp s18, s20, [x0, #8]
; VBITS_GE_256-NEXT:    sshll v22.8h, v22.8b, #0
; VBITS_GE_256-NEXT:    sshll v6.8h, v6.8b, #0
; VBITS_GE_256-NEXT:    sshll v7.8h, v7.8b, #0
; VBITS_GE_256-NEXT:    sshll v23.8h, v23.8b, #0
; VBITS_GE_256-NEXT:    sshll v16.8h, v16.8b, #0
; VBITS_GE_256-NEXT:    sshll v19.8h, v17.8b, #0
; VBITS_GE_256-NEXT:    sshll v14.8h, v14.8b, #0
; VBITS_GE_256-NEXT:    sshll v21.8h, v18.8b, #0
; VBITS_GE_256-NEXT:    sshll v20.8h, v20.8b, #0
; VBITS_GE_256-NEXT:    sshll v24.4s, v22.4h, #0
; VBITS_GE_256-NEXT:    ldp s26, s22, [x1, #32]
; VBITS_GE_256-NEXT:    sshll v17.4s, v6.4h, #0
; VBITS_GE_256-NEXT:    sshll v18.4s, v7.4h, #0
; VBITS_GE_256-NEXT:    sshll v6.4s, v16.4h, #0
; VBITS_GE_256-NEXT:    sshll v7.4s, v19.4h, #0
; VBITS_GE_256-NEXT:    sshll v16.4s, v21.4h, #0
; VBITS_GE_256-NEXT:    sshll v19.4s, v20.4h, #0
; VBITS_GE_256-NEXT:    sshll v25.4s, v23.4h, #0
; VBITS_GE_256-NEXT:    ldp s21, s20, [x0, #24]
; VBITS_GE_256-NEXT:    sshll v23.8h, v26.8b, #0
; VBITS_GE_256-NEXT:    ldp s26, s27, [x1, #40]
; VBITS_GE_256-NEXT:    sshll v22.8h, v22.8b, #0
; VBITS_GE_256-NEXT:    ldp s28, s29, [x1, #48]
; VBITS_GE_256-NEXT:    sshll v13.4s, v13.4h, #0
; VBITS_GE_256-NEXT:    ldp s30, s9, [x1, #56]
; VBITS_GE_256-NEXT:    sshll v21.8h, v21.8b, #0
; VBITS_GE_256-NEXT:    sshll v26.8h, v26.8b, #0
; VBITS_GE_256-NEXT:    sshll v20.8h, v20.8b, #0
; VBITS_GE_256-NEXT:    sshll v27.8h, v27.8b, #0
; VBITS_GE_256-NEXT:    sshll v28.8h, v28.8b, #0
; VBITS_GE_256-NEXT:    sshll v31.8h, v29.8b, #0
; VBITS_GE_256-NEXT:    sshll v14.4s, v14.4h, #0
; VBITS_GE_256-NEXT:    sshll v30.8h, v30.8b, #0
; VBITS_GE_256-NEXT:    sshll v29.4s, v21.4h, #0
; VBITS_GE_256-NEXT:    sshll v21.4s, v22.4h, #0
; VBITS_GE_256-NEXT:    sshll v22.4s, v26.4h, #0
; VBITS_GE_256-NEXT:    sshll v8.4s, v20.4h, #0
; VBITS_GE_256-NEXT:    sshll v20.4s, v23.4h, #0
; VBITS_GE_256-NEXT:    sshll v26.4s, v28.4h, #0
; VBITS_GE_256-NEXT:    sshll v23.4s, v27.4h, #0
; VBITS_GE_256-NEXT:    sshll v27.4s, v31.4h, #0
; VBITS_GE_256-NEXT:    sshll v28.4s, v30.4h, #0
; VBITS_GE_256-NEXT:    ldp s12, s30, [x1, #12]
; VBITS_GE_256-NEXT:    ldp s31, s10, [x1, #24]
; VBITS_GE_256-NEXT:    sshll v9.8h, v9.8b, #0
; VBITS_GE_256-NEXT:    ldp s15, s11, [x1, #4]
; VBITS_GE_256-NEXT:    sabd v25.4s, v25.4s, v13.4s
; VBITS_GE_256-NEXT:    sshll v30.8h, v30.8b, #0
; VBITS_GE_256-NEXT:    sshll v12.8h, v12.8b, #0
; VBITS_GE_256-NEXT:    sabd v5.4s, v5.4s, v27.4s
; VBITS_GE_256-NEXT:    sshll v10.8h, v10.8b, #0
; VBITS_GE_256-NEXT:    sshll v31.8h, v31.8b, #0
; VBITS_GE_256-NEXT:    sshll v1.4s, v9.4h, #0
; VBITS_GE_256-NEXT:    sshll v15.8h, v15.8b, #0
; VBITS_GE_256-NEXT:    sshll v11.8h, v11.8b, #0
; VBITS_GE_256-NEXT:    sabd v4.4s, v4.4s, v26.4s
; VBITS_GE_256-NEXT:    sshll v0.4s, v30.4h, #0
; VBITS_GE_256-NEXT:    sshll v12.4s, v12.4h, #0
; VBITS_GE_256-NEXT:    sabd v3.4s, v3.4s, v23.4s
; VBITS_GE_256-NEXT:    sshll v10.4s, v10.4h, #0
; VBITS_GE_256-NEXT:    sshll v30.4s, v31.4h, #0
; VBITS_GE_256-NEXT:    sabd v2.4s, v2.4s, v22.4s
; VBITS_GE_256-NEXT:    sshll v31.4s, v15.4h, #0
; VBITS_GE_256-NEXT:    sshll v9.4s, v11.4h, #0
; VBITS_GE_256-NEXT:    sabd v6.4s, v6.4s, v14.4s
; VBITS_GE_256-NEXT:    sabd v24.4s, v24.4s, v0.4s
; VBITS_GE_256-NEXT:    sabd v0.4s, v18.4s, v1.4s
; VBITS_GE_256-NEXT:    sabd v1.4s, v17.4s, v28.4s
; VBITS_GE_256-NEXT:    ldp q17, q18, [sp] // 32-byte Folded Reload
; VBITS_GE_256-NEXT:    sabd v8.4s, v8.4s, v10.4s
; VBITS_GE_256-NEXT:    sabd v29.4s, v29.4s, v30.4s
; VBITS_GE_256-NEXT:    sabd v19.4s, v19.4s, v12.4s
; VBITS_GE_256-NEXT:    sabd v16.4s, v16.4s, v9.4s
; VBITS_GE_256-NEXT:    sabd v7.4s, v7.4s, v31.4s
; VBITS_GE_256-NEXT:    splice z1.s, p0, z1.s, z0.s
; VBITS_GE_256-NEXT:    splice z4.s, p0, z4.s, z5.s
; VBITS_GE_256-NEXT:    sabd v17.4s, v17.4s, v21.4s
; VBITS_GE_256-NEXT:    sabd v18.4s, v18.4s, v20.4s
; VBITS_GE_256-NEXT:    splice z2.s, p0, z2.s, z3.s
; VBITS_GE_256-NEXT:    splice z29.s, p0, z29.s, z8.s
; VBITS_GE_256-NEXT:    splice z24.s, p0, z24.s, z25.s
; VBITS_GE_256-NEXT:    splice z16.s, p0, z16.s, z19.s
; VBITS_GE_256-NEXT:    splice z6.s, p0, z6.s, z7.s
; VBITS_GE_256-NEXT:    ldp d9, d8, [sp, #80] // 16-byte Folded Reload
; VBITS_GE_256-NEXT:    splice z18.s, p0, z18.s, z17.s
; VBITS_GE_256-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_GE_256-NEXT:    uzp1 z3.h, z4.h, z4.h
; VBITS_GE_256-NEXT:    uzp1 z2.h, z2.h, z2.h
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    uzp1 z0.h, z29.h, z29.h
; VBITS_GE_256-NEXT:    uzp1 z5.h, z24.h, z24.h
; VBITS_GE_256-NEXT:    uzp1 z7.h, z16.h, z16.h
; VBITS_GE_256-NEXT:    uzp1 z6.h, z6.h, z6.h
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    uzp1 z3.b, z3.b, z3.b
; VBITS_GE_256-NEXT:    uzp1 z4.h, z18.h, z18.h
; VBITS_GE_256-NEXT:    ldp d11, d10, [sp, #64] // 16-byte Folded Reload
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    uzp1 z2.b, z2.b, z2.b
; VBITS_GE_256-NEXT:    uzp1 z5.b, z5.b, z5.b
; VBITS_GE_256-NEXT:    uzp1 z7.b, z7.b, z7.b
; VBITS_GE_256-NEXT:    ldp d13, d12, [sp, #48] // 16-byte Folded Reload
; VBITS_GE_256-NEXT:    uzp1 z4.b, z4.b, z4.b
; VBITS_GE_256-NEXT:    uzp1 z6.b, z6.b, z6.b
; VBITS_GE_256-NEXT:    mov v3.d[1], v1.d[0]
; VBITS_GE_256-NEXT:    mov v5.d[1], v0.d[0]
; VBITS_GE_256-NEXT:    ldp d15, d14, [sp, #32] // 16-byte Folded Reload
; VBITS_GE_256-NEXT:    mov v4.d[1], v2.d[0]
; VBITS_GE_256-NEXT:    mov v6.d[1], v7.d[0]
; VBITS_GE_256-NEXT:    splice z4.b, p0, z4.b, z3.b
; VBITS_GE_256-NEXT:    splice z6.b, p0, z6.b, z5.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z4.b }, p0, [x0, x8]
; VBITS_GE_256-NEXT:    st1b { z6.b }, p0, [x0]
; VBITS_GE_256-NEXT:    add sp, sp, #96
; VBITS_GE_256-NEXT:    ret
;
; VBITS_EQ_512-LABEL: sabd_v64i8_v64i64:
; VBITS_EQ_512:       // %bb.0:
; VBITS_EQ_512-NEXT:    ldp d0, d1, [x1, #48]
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl16
; VBITS_EQ_512-NEXT:    ldp d4, d5, [x0, #48]
; VBITS_EQ_512-NEXT:    ldp d2, d3, [x0]
; VBITS_EQ_512-NEXT:    ldp d6, d7, [x0, #32]
; VBITS_EQ_512-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_512-NEXT:    ldp d1, d16, [x1, #16]
; VBITS_EQ_512-NEXT:    mov v4.d[1], v5.d[0]
; VBITS_EQ_512-NEXT:    ldp d5, d17, [x1, #32]
; VBITS_EQ_512-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_EQ_512-NEXT:    ldp d18, d19, [x0, #16]
; VBITS_EQ_512-NEXT:    mov v6.d[1], v7.d[0]
; VBITS_EQ_512-NEXT:    ldp d20, d21, [x1]
; VBITS_EQ_512-NEXT:    mov v1.d[1], v16.d[0]
; VBITS_EQ_512-NEXT:    mov v5.d[1], v17.d[0]
; VBITS_EQ_512-NEXT:    sabd v0.16b, v4.16b, v0.16b
; VBITS_EQ_512-NEXT:    mov v18.d[1], v19.d[0]
; VBITS_EQ_512-NEXT:    mov v20.d[1], v21.d[0]
; VBITS_EQ_512-NEXT:    sabd v3.16b, v6.16b, v5.16b
; VBITS_EQ_512-NEXT:    sabd v1.16b, v18.16b, v1.16b
; VBITS_EQ_512-NEXT:    sabd v2.16b, v2.16b, v20.16b
; VBITS_EQ_512-NEXT:    splice z3.b, p0, z3.b, z0.b
; VBITS_EQ_512-NEXT:    splice z2.b, p0, z2.b, z1.b
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl32
; VBITS_EQ_512-NEXT:    splice z2.b, p0, z2.b, z3.b
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl64
; VBITS_EQ_512-NEXT:    st1b { z2.b }, p0, [x0]
; VBITS_EQ_512-NEXT:    ret
;
; VBITS_GE_2048-LABEL: sabd_v64i8_v64i64:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.d, vl32
; VBITS_GE_2048-NEXT:    mov w8, #32 // =0x20
; VBITS_GE_2048-NEXT:    ld1sb { z0.d }, p0/z, [x0, x8]
; VBITS_GE_2048-NEXT:    ld1sb { z1.d }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1sb { z2.d }, p0/z, [x1, x8]
; VBITS_GE_2048-NEXT:    ld1sb { z3.d }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    sabd z0.d, p0/m, z0.d, z2.d
; VBITS_GE_2048-NEXT:    sabd z1.d, p0/m, z1.d, z3.d
; VBITS_GE_2048-NEXT:    ptrue p0.b, vl32
; VBITS_GE_2048-NEXT:    uzp1 z0.s, z0.s, z0.s
; VBITS_GE_2048-NEXT:    uzp1 z1.s, z1.s, z1.s
; VBITS_GE_2048-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_GE_2048-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_GE_2048-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_2048-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_2048-NEXT:    splice z1.b, p0, z1.b, z0.b
; VBITS_GE_2048-NEXT:    ptrue p0.b, vl64
; VBITS_GE_2048-NEXT:    st1b { z1.b }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %a.ld = load <64 x i8>, ptr %a
  %b.ld = load <64 x i8>, ptr %b
  %a.sext = sext <64 x i8> %a.ld to <64 x i64>
  %b.sext = sext <64 x i8> %b.ld to <64 x i64>
  %sub = sub <64 x i64> %a.sext, %b.sext
  %abs = call <64 x i64> @llvm.abs.v64i64(<64 x i64> %sub, i1 true)
  %trunc = trunc <64 x i64> %abs to <64 x i8>
  store <64 x i8> %trunc, ptr %a
  ret void
}

attributes #0 = { "target-features"="+neon,+sve" }
